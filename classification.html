<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clasificaci√≥n con Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <style>
        :root {
            --primary-dark: #04142b;
            --primary-medium: #142a45;
            --accent-green: #00ff85;
            --text-light: #e6e8ea;
            --text-grey: #a3a8ae;
            --progress-bg: #1f3754;
            --code-bg: #1e1e1e;
            --output-border: #b07ff3;
            
            /* Colores para los recuadros informativos */
            --note-bg: #c5e0ff;
            --note-border: #0d47a1;
            --warning-bg: #ffe082;
            --warning-border: #e65100;
            --tip-bg: #b9f6ca;
            --tip-border: #1b5e20;
            --info-text: #222222;
        }
        
        body {
            font-family: 'Inter', 'Segoe UI', system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            color: var(--text-light);
            background-color: var(--primary-dark);
        }
        
        .course-header {
            background-color: var(--primary-dark);
            padding: 1.5rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        .course-label {
            color: var(--text-grey);
            font-size: 0.9rem;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 0.5rem;
        }
        
        .course-title {
            color: var(--text-light);
            font-size: 2rem;
            font-weight: 700;
            margin: 0.5rem 0;
        }
        
        .course-author {
            color: var(--text-grey);
            font-size: 1rem;
            margin-top: 0.5rem;
        }
        
        .progress-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: 1.5rem 0;
        }
        
        .progress-bar {
            flex-grow: 1;
            height: 6px;
            background-color: var(--progress-bg);
            border-radius: 3px;
            margin-right: 15px;
            position: relative;
        }
        
        .progress-fill {
            height: 100%;
            width: 0%;
            background-color: var(--accent-green);
            border-radius: 3px;
        }
        
        .progress-info {
            display: flex;
            align-items: center;
            color: var(--text-grey);
            font-size: 0.9rem;
        }
        
        .progress-info i {
            margin-right: 5px;
        }
        
        .btn {
            background-color: var(--accent-green);
            color: var(--primary-dark);
            border: none;
            border-radius: 6px;
            padding: 0.8rem 1.8rem;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        
        .btn:hover {
            opacity: 0.9;
            transform: translateY(-1px);
        }
        
        .btn-practice {
            background-color: transparent;
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: var(--text-light);
            display: flex;
            align-items: center;
            padding: 0.6rem 1.2rem;
        }
        
        .btn-practice i {
            margin-right: 8px;
            color: #ff8a00;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        nav {
            background-color: var(--primary-medium);
            padding: 1rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
        }
        
        nav ul {
            list-style-type: none;
            margin: 0;
            padding: 0;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        nav li {
            margin-bottom: 0.5rem;
        }
        
        nav a {
            color: var(--text-grey);
            text-decoration: none;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.2s ease;
        }
        
        nav a:hover, nav a.active {
            color: var(--text-light);
            background-color: rgba(255, 255, 255, 0.1);
        }
        
        nav a.active {
            border-left: 3px solid var(--accent-green);
        }
        
        .content-section {
            background-color: var(--primary-medium);
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
        }
        
        h1, h2, h3 {
            color: var(--text-light);
            font-weight: 700;
        }
        
        h1 {
            font-size: 2.2rem;
            margin-bottom: 1.5rem;
        }
        
        h2 {
            font-size: 1.8rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            color: var(--accent-green);
        }
        
        code {
            font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
            background-color: rgba(0, 0, 0, 0.3);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            color: #e6e6e6;
        }
        
        /* Para c√≥digo dentro de los bloques de informaci√≥n con texto oscuro */
        .note code, .warning code, .tip code {
            background-color: rgba(0, 0, 0, 0.1);
            color: var(--info-text);
            font-weight: 500;
        }
        
        /* Estilos para bloques de c√≥digo */
        pre {
            background-color: var(--code-bg) !important;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
            position: relative;
            border-left: 3px solid var(--accent-green);
            line-height: 1.5;
        }
        
        pre code,
        pre code.hljs,
        .hljs {
            background-color: var(--code-bg) !important;
            padding: 0;
            color: #d4d4d4;
        }
        
        /* Forzar que no haya ning√∫n fondo en los elementos dentro del c√≥digo */
        pre *, pre code *, pre code.hljs * {
            background-color: transparent !important;
        }
        
        /* Mantener colores de sintaxis */
        .hljs-comment {
            color: #6a9955 !important;
            background-color: transparent !important;
        }
        
        .hljs-keyword, .hljs-built_in, .hljs-literal {
            color: #ff7b72 !important;
            background-color: transparent !important;
        }
        
        .hljs-string {
            color: #ce9178 !important;
            background-color: transparent !important;
        }
        
        .hljs-number {
            color: #b5cea8 !important;
            background-color: transparent !important;
        }
        
        .hljs-function, .hljs-title.function_ {
            color: #dcdcaa !important;
            background-color: transparent !important;
        }
        
        .hljs-variable {
            color: #9cdcfe !important;
            background-color: transparent !important;
        }
        
        .output {
            background-color: #000000;
            border-left: 4px solid var(--output-border);
            padding: 1rem;
            margin: 1rem 0;
            color: #f1f1f1;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
            border-radius: 0 8px 8px 0;
            box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.6);
        }
        
        /* Estilos para recuadros informativos */
        .note, .warning, .tip {
            border-radius: 6px;
            padding: 0.8rem 1.2rem;
            margin: 1.5rem 0;
            position: relative;
            font-size: 0.95rem;
            line-height: 1.5;
            border-left-width: 6px;
            border-left-style: solid;
            color: var(--info-text);
        }
        
        /* Estilo para NOTA - azul */
        .note {
            background-color: var(--note-bg);
            border-left-color: var(--note-border);
        }
        
        .note::before {
            content: "Nota:";
            font-weight: 700;
            color: var(--note-border);
            margin-right: 0.3rem;
        }
        
        /* Estilo para ADVERTENCIA - amarillo/naranja */
        .warning {
            background-color: var(--warning-bg);
            border-left-color: var(--warning-border);
        }
        
        .warning::before {
            content: "Advertencia:";
            font-weight: 700;
            color: var(--warning-border);
            margin-right: 0.3rem;
        }
        
        /* Estilo para TIP - verde */
        .tip {
            background-color: var(--tip-bg);
            border-left-color: var(--tip-border);
        }
        
        .tip::before {
            content: "Tip:";
            font-weight: 700;
            color: var(--tip-border);
            margin-right: 0.3rem;
        }
        
        .welcome-message {
            font-size: 1.1rem;
            line-height: 1.7;
            margin-bottom: 2rem;
            color: var(--text-grey);
        }
        
        .highlight {
            color: var(--accent-green);
            font-weight: 600;
        }
        
        .icon-clock:before {
            content: "‚è±Ô∏è";
            margin-right: 5px;
        }
        
        .icon-dumbbell:before {
            content: "üèãÔ∏è";
            margin-right: 5px;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding: 1rem;
            color: var(--text-grey);
            border-top: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        /* Estilos para im√°genes */
        .img-container {
            margin: 2rem 0;
            text-align: center;
        }
        
        .img-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
        }
        
        .caption {
            color: var(--text-grey);
            font-size: 0.9rem;
            margin-top: 0.5rem;
            font-style: italic;
        }

        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .course-title {
                font-size: 1.6rem;
            }
            
            nav ul {
                flex-direction: column;
            }
        }

        .table-container {
            margin: 2rem auto;
            max-width: 800px; /* Ancho m√°ximo para centrar */
        }

        .builtin-functions {
            width: 100%;
            border-collapse: collapse;
            background-color: var(--primary-medium);
            color: var(--text-light);
            margin: 0 auto; /* Centrar la tabla */
        }

        .builtin-functions tr:nth-child(odd) {
            background-color: var(--primary-medium); /* Primer tono de azul */
        }

        .builtin-functions tr:nth-child(even) {
            background-color: rgba(30, 60, 100, 0.6); /* Segundo tono de azul m√°s claro */
        }

        .builtin-functions td {
            padding: 10px 15px;
            text-align: left;
            border: none; /* Eliminar bordes laterales */
            border-bottom: 1px solid rgba(255, 255, 255, 0.2); /* L√≠nea horizontal blanca */
        }

        /* Doble l√≠nea en la parte superior de la primera fila */
        .builtin-functions tr:first-child td {
            border-top: 3px double rgba(255, 255, 255, 0.4);
        }

        .builtin-functions code {
            font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
            color: var(--accent-green);
        }

        .builtin-functions tr:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .resources-list {
            margin: 1.5rem 0;
        }

        .resources-list li {
            margin-bottom: 1rem;
            line-height: 1.6;
        }

        .resources-list strong {
            color: var(--accent-green);
        }

        .resources-list {
            margin: 1.5rem 0;
        }
        
        .resources-list li {
            margin-bottom: 1rem;
            line-height: 1.6;
        }
        
        .resources-list strong {
            color: var(--accent-green);
        }
        
        .resources-list a {
            color: var(--accent-green) !important; /* Forzar color verde */
            text-decoration: none;
            transition: all 0.2s ease;
        }
        
        .resources-list a:hover {
            text-decoration: underline;
            opacity: 0.9; /* Ligero cambio de opacidad al pasar el cursor */
        }

        .feedback-section {
            background-color: var(--primary-medium);
            padding: 1.5rem;
            border-radius: 10px;
            margin: 2rem 0;
            text-align: center;
        }

        .feedback-section h3 {
            color: var(--text-light);
            margin-bottom: 1.2rem;
        }

        .feedback-buttons {
            display: flex;
            justify-content: center;
            gap: 1rem;
        }

        .feedback-btn {
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 0.6rem 1.5rem;
            font-size: 1rem;
            border-radius: 6px;
            transition: all 0.2s ease;
            cursor: pointer;
        }

        .feedback-yes {
            background-color: rgba(0, 255, 133, 0.2);
            border: 1px solid var(--accent-green);
            color: var(--accent-green);
        }

        .feedback-no {
            background-color: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.3);
            color: var(--text-light);
        }

        .feedback-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        .feedback-icon {
            margin-right: 8px;
            font-size: 1.2rem;
        }

        .license-icons {
            display: inline-flex;
            gap: 0.5rem;
            margin-left: 1rem;
            align-items: center;
        }

        .license-link {
            display: flex;
            gap: 0.5rem;
            text-decoration: none;
        }

        .license-icon {
            width: 28px;
            height: 28px;
            transition: transform 0.2s ease;
        }

        .icon-tooltip {
            position: relative;
            cursor: pointer;
        }

        .icon-tooltip:hover .license-icon {
            transform: scale(1.1);
        }

        .icon-tooltip::after {
            content: attr(data-tooltip);
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            background-color: var(--primary-dark);
            color: var(--text-light);
            padding: 0.3rem 0.6rem;
            border-radius: 4px;
            font-size: 0.8rem;
            white-space: nowrap;
            opacity: 0;
            pointer-events: none;
            transition: opacity 0.2s ease;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.3);
            z-index: 100;
        }

        .icon-tooltip:hover::after {
            opacity: 1;
        }

        .hamburger-menu {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
        }

        .hamburger-button {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            width: 40px;
            height: 35px;
            background-color: var(--primary-medium);
            border: none;
            border-radius: 5px;
            padding: 8px;
            cursor: pointer;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
        }

        .bar {
            height: 3px;
            width: 100%;
            background-color: var(--accent-green);
            border-radius: 2px;
            transition: all 0.3s ease;
        }

        .menu-content {
            position: absolute;
            right: 0;
            top: 50px;
            width: 250px;
            background-color: var(--primary-medium);
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            padding: 1.5rem;
            transform: scale(0.95);
            transform-origin: top right;
            opacity: 0;
            visibility: hidden;
            transition: all 0.2s ease;
            max-height: 80vh;
            overflow-y: auto;
        }

        .menu-content h3 {
            color: var(--accent-green);
            margin-top: 0;
            margin-bottom: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            padding-bottom: 0.5rem;
        }

        .menu-content ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .menu-content li {
            margin-bottom: 0.7rem;
        }

        .menu-content a {
            color: var(--text-light);
            text-decoration: none;
            display: block;
            padding: 0.5rem;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .menu-content a:hover {
            background-color: rgba(255, 255, 255, 0.1);
            color: var(--accent-green);
        }

        /* Clase para mostrar/ocultar el men√∫ */
        .hamburger-menu.active .menu-content {
            transform: scale(1);
            opacity: 1;
            visibility: visible;
        }

        /* Animaci√≥n de las barras cuando est√° activo */
        .hamburger-menu.active .bar:nth-child(1) {
            transform: translateY(10px) rotate(45deg);
        }

        .hamburger-menu.active .bar:nth-child(2) {
            opacity: 0;
        }

        .hamburger-menu.active .bar:nth-child(3) {
            transform: translateY(-10px) rotate(-45deg);
        }

        /* Estilos para navegaci√≥n entre cap√≠tulos */
        .chapter-navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 2rem;
            padding-top: 1.5rem;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
        }

        .nav-btn {
            display: inline-flex;
            align-items: center;
            padding: 0.8rem 1.5rem;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 600;
            transition: all 0.2s ease;
            background-color: var(--primary-medium);
            color: var(--text-light);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .nav-btn:hover {
            background-color: var(--accent-green);
            color: var(--primary-dark);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        .nav-icon {
            font-size: 0.9rem;
            margin: 0 0.5rem;
        }

        .nav-prev .nav-icon {
            margin-right: 0.5rem;
            margin-left: 0;
        }

        .nav-next .nav-icon {
            margin-left: 0.5rem;
            margin-right: 0;
        }

        /* Para p√°ginas con un solo bot√≥n de navegaci√≥n (como la primera) */
        .chapter-navigation.single-button {
            justify-content: flex-end; /* Alinea el contenido al extremo derecho */
        }

        /* En pantallas peque√±as, ajustar la navegaci√≥n para mejor visualizaci√≥n */
        @media (max-width: 768px) {
            .chapter-navigation {
                flex-direction: column;
                gap: 1rem;
            }

            .nav-btn {
                text-align: center;
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    <div class="hamburger-menu">
        <button class="hamburger-button" aria-label="Abrir men√∫ de navegaci√≥n">
            <span class="bar"></span>
            <span class="bar"></span>
            <span class="bar"></span>
        </button>
        
        <div class="menu-content">
            <h3>Secciones</h3>
            <ul>
                <li><a href="#intro-clasificacion">Introducci√≥n a la Clasificaci√≥n</a></li>
                <li><a href="#algoritmos-basicos">Algoritmos B√°sicos</a></li>
                <li><a href="#metricas-evaluacion">M√©tricas de Evaluaci√≥n</a></li>
                <li><a href="#etapa-2-exploracion">Exploraci√≥n de Datos</a></li>
                <li><a href="#etapa-3-limpieza">Limpieza de Datos</a></li>
                <li><a href="#etapa-4-conversion">Conversi√≥n de Datos</a></li>
                <li><a href="#etapa-5-exploracion">Exploraci√≥n de Datos</a></li>
                <li><a href="#etapa-6-feature-engineering">Feature Engineering</a></li>
                <li><a href="#etapa-8-entrenamiento">Entrenamiento del Modelo</a></li>
                <li><a href="#etapa-9-validacion">Validaci√≥n del Modelo</a></li>
                <li><a href="#etapa-10-optimizacion">Optimizaci√≥n del Modelo</a></li>
                <li><a href="#etapa-11-evaluacion">Evaluaci√≥n del Modelo</a></li>
                <li><a href="#etapa-12-interpretacion">Interpretaci√≥n de Resultados</a></li>
                <li><a href="#etapa-13-preparacion">Despliegue</a></li>
                <li><a href="#material-practica">Material de Pr√°ctica</a></li>
                <li><a href="#referencias">Referencias</a></li>
            </ul>
        </div>
    </div>
     
    <div class="course-header">
        <div class="container">
            <div class="course-label">CURSO</div>
            <h1 class="course-title">Python para Ciencia de Datos</h1>
            <div class="course-author">Ph.D. Antonio Escamilla P.</div>
            
            <div class="progress-container">
                <div class="progress-bar">
                    <div class="progress-fill" style="width: 87.5%;"></div>
                </div>
                <div class="progress-info">
                    <span class="icon-clock"></span>
                    2 sections to go
                </div>
            </div>
            
            <div style="display: flex; justify-content: space-between;">
                <button class="btn btn-practice" onclick="document.getElementById('material-practica').scrollIntoView({behavior: 'smooth'})">
                    <span class="icon-dumbbell"></span>
                    Practica
                </button>
                <button class="btn" onclick="window.location.href='clustering.html'">Continuar</button>
            </div>
        </div>
    </div>
     
    <div class="container">
        <nav>
            <ul>
                <li><a href="index.html">1. Introducci√≥n a Python</a></li>
                <li><a href="numpy.html">2. NumPy</a></li>
                <li><a href="pandas.html">3. Pandas</a></li>
                <li><a href="polars.html">4. Polars</a></li>
                <li><a href="visualization.html">5. Visualizaci√≥n</a></li>
                <li><a href="machine_learning.html">6. Machine Learning</a></li>
                <li><a href="regression.html">7. Regresi√≥n</a></li>
                <li><a href="classification.html" class="active">8. Clasificaci√≥n</a></li>
                <li><a href="clustering.html">9. Clustering</a></li>
            </ul>
        </nav>
        
        <div class="content-section">
            <h1 id="intro-clasificacion">Cap√≠tulo 8: Algoritmos de Clasificaci√≥n en Machine Learning</h1>
            
            <h2 id="que-es-clasificacion">¬øQu√© es la Clasificaci√≥n?</h2>
            
            <p>La clasificaci√≥n es una t√©cnica de aprendizaje supervisado donde el objetivo es predecir a qu√© categor√≠a o clase pertenece una observaci√≥n. A diferencia de la regresi√≥n que predice valores continuos, la clasificaci√≥n asigna etiquetas discretas o categor√≠as.</p>
            
            <div class="img-container">
                <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png" alt="Comparaci√≥n de algoritmos de clasificaci√≥n">
                <div class="caption">Comparaci√≥n visual de diferentes algoritmos de clasificaci√≥n (Fuente: Scikit-learn)</div>
            </div>
            
            <p>Los modelos de clasificaci√≥n son ampliamente utilizados en numerosos campos:</p>
            
            <ul>
                <li><strong>Medicina</strong>: Diagn√≥stico de enfermedades, identificaci√≥n de c√©lulas cancer√≠genas</li>
                <li><strong>Finanzas</strong>: Detecci√≥n de fraudes, evaluaci√≥n de riesgos crediticios</li>
                <li><strong>Marketing</strong>: Segmentaci√≥n de clientes, predicci√≥n de comportamientos de compra</li>
                <li><strong>Tecnolog√≠a</strong>: Filtros de spam, reconocimiento facial, an√°lisis de sentimientos</li>
                <li><strong>Industria</strong>: Control de calidad, mantenimiento predictivo</li>
            </ul>
            
            <h2 id="tipos-clasificacion">Tipos de Problemas de Clasificaci√≥n</h2>
            
            <p>Existen diferentes tipos de problemas de clasificaci√≥n, dependiendo de la naturaleza de las clases:</p>
            
            <h3>Clasificaci√≥n Binaria</h3>
            
            <p>En la clasificaci√≥n binaria, el objetivo es asignar cada observaci√≥n a una de dos clases posibles.</p>
            
            <p>Ejemplos:</p>
            <ul>
                <li>Correo electr√≥nico (spam / no spam)</li>
                <li>Diagn√≥stico m√©dico (enfermo / sano)</li>
                <li>Aprobaci√≥n de cr√©dito (aprobado / rechazado)</li>
            </ul>
            
            <h3>Clasificaci√≥n Multiclase</h3>
            
            <p>En la clasificaci√≥n multiclase, el objetivo es asignar cada observaci√≥n a una de varias clases posibles, donde cada observaci√≥n pertenece exactamente a una clase.</p>
            
            <p>Ejemplos:</p>
            <ul>
                <li>Clasificaci√≥n de especies de plantas</li>
                <li>Reconocimiento de d√≠gitos manuscritos (0-9)</li>
                <li>Categorizaci√≥n de productos en un cat√°logo</li>
            </ul>
            
            <h3>Clasificaci√≥n Multilabel</h3>
            
            <p>En la clasificaci√≥n multilabel, cada observaci√≥n puede pertenecer a m√∫ltiples clases simult√°neamente.</p>
            
            <p>Ejemplos:</p>
            <ul>
                <li>Etiquetado de im√°genes (puede contener "persona", "coche", "√°rbol")</li>
                <li>Categorizaci√≥n de art√≠culos de noticias (puede ser "pol√≠tica" y "econom√≠a")</li>
                <li>Clasificaci√≥n de pel√≠culas por g√©nero (puede ser "drama" y "rom√°ntica")</li>
            </ul>
            
            <div class="note">
                La elecci√≥n del algoritmo de clasificaci√≥n adecuado depende no solo del tipo de problema, sino tambi√©n de factores como el tama√±o del conjunto de datos, la dimensionalidad, la interpretabilidad requerida y el equilibrio entre precisi√≥n y velocidad.
            </div>
        </div>

        <div class="content-section">
            <h2 id="algoritmos-basicos">Algoritmos B√°sicos de Clasificaci√≥n</h2>
            
            <p>En este cap√≠tulo exploraremos los algoritmos de clasificaci√≥n m√°s utilizados en machine learning y c√≥mo aplicarlos de manera efectiva en un proyecto real. La clasificaci√≥n es una tarea supervisada donde el objetivo es predecir a qu√© clase o categor√≠a pertenece una observaci√≥n.</p>
            
            <h3>Principales Algoritmos de Clasificaci√≥n</h3>
            
            <p>Existen numerosos algoritmos de clasificaci√≥n, cada uno con sus fortalezas y debilidades. Los m√°s utilizados incluyen:</p>
            
            <ul>
                <li><strong>Regresi√≥n Log√≠stica</strong>: Un algoritmo lineal que calcula la probabilidad de pertenencia a una clase utilizando la funci√≥n log√≠stica.</li>
                <li><strong>√Årboles de Decisi√≥n</strong>: Modelos que dividen los datos en subconjuntos bas√°ndose en el valor de las caracter√≠sticas, formando una estructura similar a un √°rbol.</li>
                <li><strong>Random Forest</strong>: Conjunto de √°rboles de decisi√≥n cuyas predicciones se combinan para mejorar la precisi√≥n y reducir el sobreajuste.</li>
                <li><strong>Support Vector Machines</strong>: Algoritmos que buscan un hiperplano √≥ptimo para separar las clases en el espacio de caracter√≠sticas.</li>
                <li><strong>Naive Bayes</strong>: Clasificadores probabil√≠sticos basados en el teorema de Bayes, que asumen independencia entre las caracter√≠sticas.</li>
                <li><strong>K-Nearest Neighbors</strong>: Clasificadores basados en la proximidad, que asignan la clase m√°s com√∫n entre los k ejemplos m√°s cercanos.</li>
                <li><strong>Gradient Boosting</strong>: T√©cnicas de ensamblaje que construyen modelos secuencialmente, donde cada uno intenta corregir los errores del anterior.</li>
                <li><strong>Redes Neuronales</strong>: Modelos inspirados en el cerebro humano capaces de aprender representaciones complejas.</li>
            </ul>

            <p>La elecci√≥n del algoritmo adecuado depende de factores como la naturaleza de los datos, el tama√±o del conjunto de datos, la velocidad necesaria para la predicci√≥n, la interpretabilidad requerida y la complejidad del problema.</p>
        </div>

        <div class="content-section">
            <h2 id="metricas-evaluacion">M√©tricas de Evaluaci√≥n para Clasificaci√≥n</h2>
            
            <p>Para evaluar el rendimiento de los modelos de clasificaci√≥n, disponemos de diversas m√©tricas, cada una enfocada en diferentes aspectos del desempe√±o:</p>
            
            <h3>Matriz de Confusi√≥n</h3>
            
            <p>Es una tabla que describe el rendimiento de un modelo de clasificaci√≥n. Para un clasificador binario, contiene:</p>
            
            <ul>
                <li><strong>Verdaderos Positivos (TP)</strong>: Casos positivos correctamente clasificados</li>
                <li><strong>Falsos Positivos (FP)</strong>: Casos negativos incorrectamente clasificados como positivos</li>
                <li><strong>Falsos Negativos (FN)</strong>: Casos positivos incorrectamente clasificados como negativos</li>
                <li><strong>Verdaderos Negativos (TN)</strong>: Casos negativos correctamente clasificados</li>
            </ul>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20IA/confusion_matrix.png" alt="Matriz de confusi√≥n" width="60%">
                <div class="caption">Ejemplo de matriz de confusi√≥n visualizada (Fuente: Scikit-learn)</div>
            </div>

            <h3>M√©tricas derivadas de la Matriz de Confusi√≥n</h3>

            <ul>
                <li><strong>Exactitud (Accuracy)</strong>: Proporci√≥n de predicciones correctas.
                    <br>Accuracy = (TP + TN) / (TP + TN + FP + FN)</li>
                <li><strong>Precisi√≥n</strong>: De los casos predichos como positivos, cu√°ntos son realmente positivos.
                    <br>Precision = TP / (TP + FP)</li>
                <li><strong>Sensibilidad o Recall</strong>: De los casos realmente positivos, cu√°ntos fueron identificados correctamente.
                    <br>Recall = TP / (TP + FN)</li>
                <li><strong>F1-Score</strong>: Media arm√≥nica entre precisi√≥n y recall.
                    <br>F1 = 2 * (Precision * Recall) / (Precision + Recall)</li>
            </ul>

            <div class="tip">
                La elecci√≥n de la m√©trica adecuada depende del contexto del problema. Por ejemplo, en detecci√≥n de fraudes, la precisi√≥n puede ser m√°s importante (minimizar falsos positivos), mientras que en diagn√≥stico m√©dico, el recall puede ser prioritario (minimizar falsos negativos).
            </div>
        </div>

        <div class="content-section">
            <h2 id="workflow-clasificacion">Workflow Completo de Clasificaci√≥n</h2>
            
            <p>A continuaci√≥n, desarrollaremos un workflow completo para un problema de clasificaci√≥n utilizando el conocido dataset del Titanic. Este conjunto de datos contiene informaci√≥n sobre los pasajeros del Titanic y el objetivo es predecir si un pasajero sobrevivi√≥ o no al naufragio.</p>
            
            <div class="note">
                Este workflow sigue las mejores pr√°cticas en ciencia de datos e incluye todas las etapas necesarias: desde la carga y preparaci√≥n de datos hasta la evaluaci√≥n e interpretaci√≥n del modelo final.
            </div>

            <h3 id="etapa-1-importacion">Etapa 1: Importaci√≥n de Librer√≠as y Datos</h3>

            <p>Comenzamos importando las librer√≠as necesarias y cargando el conjunto de datos del Titanic:</p>
            
<pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset del Titanic
titanic_df = pd.read_csv("https://www.openml.org/data/get_csv/16826755/phpMYEkMl")

# Visualizar una muestra de los datos
titanic_df.sample(5)</code></pre>
            
            <p>El dataset contiene informaci√≥n como la clase del pasajero (pclass), si sobrevivi√≥ o no (survived), nombre, sexo, edad, n√∫mero de hermanos/c√≥nyuges a bordo (sibsp), n√∫mero de padres/hijos a bordo (parch), n√∫mero de ticket, tarifa (fare), cabina, puerto de embarque, y m√°s.</p>
            
            <h3 id="etapa-2-exploracion">Etapa 2: Exploraci√≥n Inicial de los Datos</h3>
            
            <p>Antes de comenzar con el procesamiento, es importante entender la estructura y caracter√≠sticas de nuestros datos:</p>
            
<pre><code class="language-python"># Obtener informaci√≥n sobre el dataset
titanic_df.info()</code></pre>
            
<pre class="output"><code class="language-shell"><class 'pandas.core.frame.DataFrame'>
RangeIndex: 1309 entries, 0 to 1308
Data columns (total 14 columns):
 #   Column     Non-Null Count  Dtype 
---  ------     --------------  ----- 
 0   pclass     1309 non-null   int64 
 1   survived   1309 non-null   int64 
 2   name       1309 non-null   object
 3   sex        1309 non-null   object
 4   age        1309 non-null   object
 5   sibsp      1309 non-null   int64 
 6   parch      1309 non-null   int64 
 7   ticket     1309 non-null   object
 8   fare       1309 non-null   object
 9   cabin      1309 non-null   object
 10  embarked   1309 non-null   object
 11  boat       1309 non-null   object
 12  body       1309 non-null   object
 13  home.dest  1309 non-null   object
dtypes: int64(4), object(10)
memory usage: 143.3+ KB</code></pre>
            
            <p>Observamos que el conjunto de datos contiene 1,309 registros con 14 columnas. Algunos campos como 'age' y 'fare' aparecen como tipo 'object' cuando deber√≠an ser num√©ricos, lo que indica posibles problemas con los datos.</p>
            
            <h3 id="etapa-3-limpieza">Etapa 3: Limpieza y Preparaci√≥n de Datos</h3>
            
            <p>Para simplificar nuestro an√°lisis, eliminaremos algunas columnas que no son relevantes para la predicci√≥n o que podr√≠an causar filtraciones de datos (como 'boat' y 'body', que contienen informaci√≥n posterior al naufragio):</p>
            
<pre><code class="language-python"># Eliminar columnas no necesarias para la predicci√≥n
titanic_df = titanic_df.drop(['boat', 'body', 'home.dest', 'name', 'ticket', 'cabin'], axis='columns')
titanic_df.head()</code></pre>
            
            <p>Ahora tenemos un conjunto de datos m√°s manejable con s√≥lo las caracter√≠sticas esenciales: 'pclass', 'survived', 'sex', 'age', 'sibsp', 'parch', 'fare' y 'embarked'.</p>
            
            <p>A continuaci√≥n, identificamos y tratamos los valores faltantes:</p>
            
<pre><code class="language-python"># En este dataset, los valores faltantes est√°n representados como '?'
titanic_df = titanic_df.replace('?', np.nan)

# Verificar valores nulos por columna
print(titanic_df.isna().sum())</code></pre>
            
<pre class="output"><code class="language-shell">pclass       0
survived     0
sex          0
age        263
sibsp        0
parch        0
fare         1
embarked     2
dtype: int64</code></pre>
            
            <p>Vemos que tenemos valores faltantes principalmente en la columna 'age' (263), y algunos en 'fare' (1) y 'embarked' (2).</p>
        </div>

        <div class="content-section">
            <h3 id="etapa-4-conversion">Etapa 4: Conversi√≥n de Tipos de Datos</h3>

            <p>Ahora que hemos identificado los valores faltantes, vamos a convertir cada columna al tipo de dato adecuado:</p>
            
<pre><code class="language-python"># Corregir las variables categ√≥ricas
cols_categoricas = ["pclass", "sex", "embarked"]
titanic_df[cols_categoricas] = titanic_df[cols_categoricas].astype("category")

# Corregir variable categ√≥rica ordinal
titanic_df["pclass"] = pd.Categorical(titanic_df["pclass"],
                                      categories=[3, 2, 1],
                                      ordered=True)

# Corregir las variables num√©ricas
cols_numericas = ["age", "fare"]
titanic_df[cols_numericas] = titanic_df[cols_numericas].astype("float")

# Corregir las variables booleanas
cols_booleanas = ["survived"]
titanic_df[cols_booleanas] = titanic_df[cols_booleanas].astype("bool")

# Verificar los tipos de datos
titanic_df.info()</code></pre>
            
<pre class="output"><code class="language-shell"><class 'pandas.core.frame.DataFrame'>
RangeIndex: 1309 entries, 0 to 1308
Data columns (total 8 columns):
 #   Column    Non-Null Count  Dtype   
---  ------    --------------  -----   
 0   pclass    1309 non-null   category
 1   survived  1309 non-null   bool    
 2   sex       1309 non-null   category
 3   age       1046 non-null   float64 
 4   sibsp     1309 non-null   int64   
 5   parch     1309 non-null   int64   
 6   fare      1308 non-null   float64 
 7   embarked  1307 non-null   category
dtypes: bool(1), category(3), float64(2), int64(2)
memory usage: 46.5 KB</code></pre>

            <h3 id="etapa-5-exploracion">Etapa 5: An√°lisis Exploratorio de Datos</h3>

            <p>Es importante explorar la distribuci√≥n de la variable objetivo para entender el balance de clases:</p>
            
<pre><code class="language-python"># Visualizar la distribuci√≥n de la variable objetivo (survived)
titanic_df["survived"].value_counts().plot(kind="bar", color=['skyblue', 'orange'])
plt.title('Distribuci√≥n de Supervivencia')
plt.xlabel('Sobrevivi√≥')
plt.ylabel('N√∫mero de pasajeros')
plt.xticks([0, 1], ['No (809)', 'S√≠ (500)'])
plt.show()</code></pre>
            
            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_1.png" alt="Distribuci√≥n de supervivencia en el Titanic">
                <div class="caption">Distribuci√≥n de la variable objetivo: sobrevivientes vs no sobrevivientes</div>
            </div>
            
            <p>Observamos que hay una distribuci√≥n desbalanceada, con m√°s pasajeros que no sobrevivieron (809) que los que s√≠ lo hicieron (500), pero no es un desbalance extremo.</p>
            
            <p>Tambi√©n es √∫til examinar la relaci√≥n entre la variable objetivo y otras caracter√≠sticas importantes:</p>
            
<pre><code class="language-python"># Relaci√≥n entre sexo y supervivencia
sns.countplot(x='sex', hue='survived', data=titanic_df)
plt.title('Supervivencia por Sexo')
plt.show()</code></pre>
            
            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_1_1.png" alt="Supervivencia por sexo">
                <div class="caption">Supervivencia seg√∫n el sexo del pasajero</div>
            </div>
            
            <p>El gr√°fico muestra claramente que el sexo fue un factor determinante en la supervivencia. Las mujeres tuvieron una tasa de supervivencia mucho mayor que los hombres, lo que refleja la pol√≠tica "mujeres y ni√±os primero" durante el desastre.</p>
            
<pre><code class="language-python"># Relaci√≥n entre clase y supervivencia
sns.countplot(x='pclass', hue='survived', data=titanic_df)
plt.title('Supervivencia por Clase')
plt.show()</code></pre>
            
            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_1_2.png" alt="Supervivencia por clase">
                <div class="caption">Supervivencia seg√∫n la clase del pasajero</div>
            </div>
            
            <p>Tambi√©n observamos una fuerte correlaci√≥n entre la clase del pasajero y su probabilidad de supervivencia. Los pasajeros de primera clase tuvieron tasas de supervivencia significativamente mayores que los de tercera clase.</p>

            <p>Analicemos visualmente c√≥mo las variables num√©ricas se relacionan con la supervivencia de pasajeros en el dataset del Titanic. Inicialmente, se utilizan diagramas de caja (boxplots) para comparar la distribuci√≥n de cada variable num√©rica entre los pasajeros que sobrevivieron y los que no. Estos gr√°ficos permiten identificar diferencias en medidas centrales (mediana), dispersi√≥n (rango intercuart√≠lico) y valores at√≠picos entre ambos grupos. </p>

            <pre><code class="language-python">fig, axes = plt.subplots(1, 2, figsize=(8, 4))
axes = axes.flatten()
for i, col in enumerate(cols_numericas):
    sns.boxplot(data=titanic_df, x="survived", y=col, ax=axes[i])
    axes[i].set_title(col)
plt.tight_layout()
plt.show()</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_2.png" alt="Num√©rica vs Supervivencia" width="70%">
                <div class="caption">An√°lisis de las variables num√©ricas frente a la variable survived</div>
            </div>

            <p>Enfoqu√©monos espec√≠ficamente en la variable 'fare' (costo del boleto), empleando gr√°ficos de densidad (KDE plots) para visualizar c√≥mo se distribuyen los precios pagados seg√∫n el estado de supervivencia. Esta visualizaci√≥n ayuda a identificar si existieron diferencias en las tarifas pagadas entre quienes sobrevivieron y quienes no, sugiriendo posibles relaciones entre el poder adquisitivo y las probabilidades de supervivencia. </p>

<pre><code class="language-python"># Gr√°fica con seaborn de la distribuci√≥n de fare por survived
sns.kdeplot(data=titanic_df, x='fare', hue='survived', alpha=0.5, fill=True);
plt.ylabel("Costo del tiquete");</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_3.png" alt="Fare vs Supervivencia" width="50%">
                <div class="caption">Distribuci√≥n del costo del boleto seg√∫n la variable survived</div>
            </div>

            <p>Analicemos ahora la relaci√≥n entre variables categ√≥ricas y la supervivencia de pasajeros en el dataset del Titanic. En el primer fragmento, se utilizan mapas de calor (heatmaps) para visualizar la distribuci√≥n conjunta de cada variable categ√≥rica con respecto a la supervivencia. Estos gr√°ficos permiten identificar r√°pidamente patrones y posibles asociaciones entre categor√≠as espec√≠ficas y la probabilidad de sobrevivir.</p>

<pre><code class="language-python"># Crear gr√°ficas de heatmap para ver la correlaci√≥n entre las variables categ√≥ricas y la variable survived
fig, axes = plt.subplots(1, 3, figsize=(12, 4))
axes = axes.flatten()
for i, col in enumerate(cols_categoricas):
    sns.heatmap(pd.crosstab(titanic_df[col],
                           titanic_df["survived"]),
               annot=True, fmt="d",
               ax=axes[i])
    axes[i].set_title(col)
plt.tight_layout()</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_4.png" alt="Categoricas vs Supervivencia" width="70%">
                <div class="caption">Correlaci√≥n entre las variables categ√≥ricas y la variable survived</div>
            </div>

            <p>Complementemos el an√°lisis visual con pruebas estad√≠sticas de chi-cuadrado, que determinan si existe una asociaci√≥n estad√≠sticamente significativa entre cada variable categ√≥rica y la supervivencia. Los valores bajos de p (p-value) indicar√≠an que la relaci√≥n observada no se debe al azar, sino que existe una dependencia real entre las variables analizadas.</p> 

<pre><code class="language-python">from scipy import stats
resultados_chi2 = []
for col in cols_categoricas:
    # Calcular la prueba chi-cuadrado
    chi2, pval, dof, expected = stats.chi2_contingency(pd.crosstab(titanic_df[col],
                                                                  titanic_df["survived"]))
    # Guardar valores en pandas dataframe para concatenar con los resultados de otras variables
    df = pd.DataFrame({'variable': [col],
                       'chi2': [chi2],
                       'pval': [pval]})
    resultados_chi2.append(df)
df_chi2 = pd.concat(resultados_chi2, ignore_index=True)
df_chi2 </code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_4_1.png" alt="resultado chi-cuadrado" width="30%">
                <div class="caption">Resultados de la prueba chi-cuadrado</div>
            </div>
        </div>

        <div class="content-section">
            <h3 id="etapa-6-feature-engineering">Etapa 6: Feature Engineering</h3>

            <p>Antes de entrenar nuestros modelos, necesitamos preparar adecuadamente nuestras caracter√≠sticas mediante t√©cnicas de preprocesamiento:</p>
            
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Definir columnas para diferentes tipos de transformaciones
cols_numericas = ["age", "fare", "sibsp", "parch"]
cols_categoricas = ["sex", "embarked"]
cols_categoricas_ord = ["pclass"]

# Pipeline para variables num√©ricas
numeric_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
])

# Pipeline para variables categ√≥ricas nominales
categorical_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder())])

# Pipeline para variables categ√≥ricas ordinales
categorical_ord_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OrdinalEncoder())])

# Combinar todos los pipelines en un preprocesador
preprocessor = ColumnTransformer(
    transformers=[
        ('numericas', numeric_pipe, cols_numericas),
        ('categoricas', categorical_pipe, cols_categoricas),
        ('categoricas ordinales', categorical_ord_pipe, cols_categoricas_ord)
    ])</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_preprocessor.png" alt="Pipeline de Preprocesamiento" width="70%">
                <div class="caption">Pipeline de Preprocesamiento</div>
            </div>
            
            <p>Este enfoque de pipelines nos permite:</p>
            <ul>
                <li>Manejar valores faltantes utilizando diferentes estrategias seg√∫n el tipo de variable</li>
                <li>Aplicar codificaci√≥n one-hot para variables categ√≥ricas nominales</li>
                <li>Aplicar codificaci√≥n ordinal para variables categ√≥ricas ordinales</li>
                <li>Asegurar que el preprocesamiento se aplique de manera consistente tanto en los datos de entrenamiento como en los de prueba</li>
            </ul>

            <h3 id="etapa-7-division">Etapa 7: Divisi√≥n de Datos y Entrenamiento Inicial</h3>

            <p>Ahora dividimos nuestros datos en conjuntos de entrenamiento y prueba, y preparamos nuestra variable objetivo:</p>
            
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# Separar caracter√≠sticas y variable objetivo
X_features = titanic_df.drop('survived', axis='columns')
y_target = titanic_df['survived']

# Dividir en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
    X_features,
    y_target,
    test_size=0.2,
    stratify=y_target,  # Mantener la misma proporci√≥n de clases
    random_state=42
)

print(f"Tama√±o de conjunto de entrenamiento: {X_train.shape[0]} ejemplos")
print(f"Tama√±o de conjunto de prueba: {X_test.shape[0]} ejemplos")</code></pre>
            
<pre class="output"><code class="language-shell">Tama√±o de conjunto de entrenamiento: 1047 ejemplos
Tama√±o de conjunto de prueba: 262 ejemplos</code></pre>

            <h3 id="etapa-8-entrenamiento">Etapa 8: Entrenamiento y Evaluaci√≥n de M√∫ltiples Modelos</h3>

            <p>Vamos a entrenar y evaluar varios modelos de clasificaci√≥n para comparar su rendimiento:</p>
            
<pre><code class="language-python">from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB

# Funci√≥n para resumir m√©tricas de clasificaci√≥n
def summarize_classification(y_test, y_pred):
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return {
        'accuracy': acc,
        'precision': prec,
        'recall': recall,
        'f1': f1
    }

# Funci√≥n para construir y evaluar un modelo
def build_model(classifier_fn, X_train, X_test, y_train, y_test):
    # Crear el pipeline con el preprocesamiento y el modelo
    pipe = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", classifier_fn)
    ])
    
    # Entrenar el modelo
    pipe.fit(X_train, y_train)
    
    # Predecir en train y test
    y_pred_train = pipe.predict(X_train)
    y_pred_test = pipe.predict(X_test)
    
    # Calcular m√©tricas
    train_metrics = summarize_classification(y_train, y_pred_train)
    test_metrics = summarize_classification(y_test, y_pred_test)
    
    return {
        'pipeline': pipe,
        'train_metrics': train_metrics,
        'test_metrics': test_metrics
    }

# Diccionario para almacenar los resultados
results = {}

# Entrenar varios modelos
print("Entrenando modelos...")

# 1. Regresi√≥n Log√≠stica
results['logistic'] = build_model(
    LogisticRegression(solver='liblinear', max_iter=1000),
    X_train, X_test, y_train, y_test
)

# 2. √Årbol de Decisi√≥n
results['decision_tree'] = build_model(
    DecisionTreeClassifier(random_state=42),
    X_train, X_test, y_train, y_test
)

# 3. An√°lisis Discriminante Lineal
results['lda'] = build_model(
    LinearDiscriminantAnalysis(),
    X_train, X_test, y_train, y_test
)

# 4. Naive Bayes
results['naive_bayes'] = build_model(
    GaussianNB(),
    X_train, X_test, y_train, y_test
)

# Comparar resultados (m√©tricas F1)
for name, result in results.items():
    print(f"{name.ljust(15)}: Train F1 = {result['train_metrics']['f1']:.4f}, Test F1 = {result['test_metrics']['f1']:.4f}")</code></pre>
            
<pre class="output"><code class="language-shell">Entrenando modelos...
logistic       : Train F1 = 0.7681, Test F1 = 0.6842
decision_tree  : Train F1 = 0.7785, Test F1 = 0.6763
lda            : Train F1 = 0.7621, Test F1 = 0.6909
naive_bayes    : Train F1 = 0.7459, Test F1 = 0.6607</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_models.png" alt="F1 score de los modelos" width="70%">
                <div class="caption">Comparaci√≥n de Modelos usando F1 score</div>
            </div>

        </div>

        <div class="content-section">
            <h3 id="etapa-9-validacion">Etapa 9: Validaci√≥n Cruzada para Selecci√≥n de Modelos</h3>

            <p>Para obtener una estimaci√≥n m√°s robusta del rendimiento de nuestros modelos, utilizamos validaci√≥n cruzada:</p>
            
<pre><code class="language-python">from sklearn.model_selection import cross_val_score

# Definir modelos a evaluar
models = [
    ('Logistic', LogisticRegression(solver='liblinear')),
    ('Decision Tree', DecisionTreeClassifier()),
    ('LDA', LinearDiscriminantAnalysis()),
    ('Naive Bayes', GaussianNB())
]

results = []
names = []
# Evaluar cada modelo con validaci√≥n cruzada
for name, model in models:
    # Crear pipeline completo
    model_pipe = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    
    # Realizar validaci√≥n cruzada
    cv_scores = cross_val_score(
        model_pipe, 
        X_train, y_train, 
        cv=10,  # 10-fold cross-validation
        scoring='f1'
    )

    results.append(cv_scores)
    names.append(name)
    
    print(f"{name}: F1 = {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")</code></pre>
            
<pre class="output"><code class="language-shell">Logistic: F1 = 0.7091 (¬±0.0581)
Decision Tree: F1 = 0.7008 (¬±0.0379)
LDA: F1 = 0.7121 (¬±0.0553)</code></pre>

            <p>La validaci√≥n cruzada nos muestra que los modelos LDA y Logistic Regression tienen el mejor rendimiento promedio, con puntajes F1 de 0.7121 y 0.7091 respectivamente.</p>

<pre><code class="language-python">plt.figure(figsize = (8,4))
result_df = pd.DataFrame(results, index=names).T
result_df.boxplot()
plt.title("Resultados de Cross Validation");

# Visualizaci√≥n de resultados de cada fold
plt.figure(figsize=(8, 4))
sns.lineplot(data=result_df)
plt.title("Resultados de cada Kfold")
plt.show()</code></pre>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_5.png" alt="Resultados de validaci√≥n cruzada">
                <div class="caption">Resultados de validaci√≥n cruzada para diferentes modelos</div>
            </div>

            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_6.png" alt="Resultados de cada K fold">
                <div class="caption">Resultados de validaci√≥n cruzada para diferentes modelos</div>
            </div>

            <h3 id="etapa-10-optimizacion">Etapa 10: Optimizaci√≥n de Hiperpar√°metros</h3>

            <p>Ahora, optimizaremos los hiperpar√°metros de nuestros mejores modelos para mejorar a√∫n m√°s su rendimiento:</p>
            
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# Optimizaci√≥n para Decision Tree
tree_params = {
    'model__max_depth': [4, 5, 7, 9, 10],
    'model__max_features': [2, 3, 4, 5, 6, 7, 8, 9],
    'model__criterion': ['gini', 'entropy'],
}

tree_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", DecisionTreeClassifier(random_state=42))
])

tree_search = GridSearchCV(
    tree_pipe,
    tree_params,
    cv=3,
    scoring='f1',
    return_train_score=True
)

tree_search.fit(X_train, y_train)

print("Mejores par√°metros para Decision Tree:")
print(tree_search.best_params_)
print(f"Mejor puntaje F1: {tree_search.best_score_:.4f}")</code></pre>
            
<pre class="output"><code class="language-shell">Mejores par√°metros para Decision Tree:
{'model__criterion': 'gini', 'model__max_depth': 9, 'model__max_features': 3}
Mejor puntaje F1: 0.7241</code></pre>
            
<pre><code class="language-python"># Optimizaci√≥n para Logistic Regression
log_params = {
    'model__C': [0.1, 0.4, 0.8, 1, 2, 5],
    'model__penalty': ['l1', 'l2'],
}

log_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", LogisticRegression(solver='liblinear'))
])

log_search = GridSearchCV(
    log_pipe,
    log_params,
    cv=3,
    scoring='f1',
    return_train_score=True
)

log_search.fit(X_train, y_train)

print("\nMejores par√°metros para Logistic Regression:")
print(log_search.best_params_)
print(f"Mejor puntaje F1: {log_search.best_score_:.4f}")</code></pre>
            
<pre class="output"><code class="language-shell">Mejores par√°metros para Logistic Regression:
{'model__C': 5, 'model__penalty': 'l1'}
Mejor puntaje F1: 0.7252</code></pre>

            <h3 id="etapa-11-evaluacion">Etapa 11: Evaluaci√≥n Final del Modelo</h3>

            <p>Tras la optimizaci√≥n de hiperpar√°metros, vamos a evaluar exhaustivamente el rendimiento de nuestro Decision Tree con la configuraci√≥n √≥ptima:</p>
            
<pre><code class="language-python">from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

# Crear el modelo optimizado de Decision Tree con los mejores hiperpar√°metros
modelo = DecisionTreeClassifier(criterion='gini',
                              max_depth=9,
                              max_features=3,
                              random_state=42)

# Pipeline completo
decision_tree_pipe = Pipeline(steps=[("preprocessor", preprocessor),
                                   ("model", modelo)])

# Entrenar el modelo final
decision_tree_model = decision_tree_pipe.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = decision_tree_model.predict(X_test)

# Generar reporte de clasificaci√≥n
print("Reporte de Clasificaci√≥n - Decision Tree:")
print(classification_report(y_test, y_pred))</code></pre>
            
<pre class="output"><code class="language-shell">Reporte de Clasificaci√≥n - Decision Tree:
              precision    recall  f1-score   support

       False       0.77      0.94      0.85       162
        True       0.86      0.55      0.67       100

    accuracy                           0.79       262
   macro avg       0.82      0.75      0.76       262
weighted avg       0.81      0.79      0.78       262</code></pre>
            
            <p>Visualizamos la matriz de confusi√≥n para entender mejor los aciertos y errores del modelo:</p>
            
<pre><code class="language-python"># Visualizar matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[False, True])
disp.plot(cmap='Blues')
plt.title('Matriz de Confusi√≥n - Decision Tree')
plt.show()</code></pre>
            
            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_7.png" alt="Matriz de confusi√≥n del Decision Tree">
                <div class="caption">Matriz de confusi√≥n del modelo Decision Tree en el conjunto de prueba</div>
            </div>
            
            <p>Analizando el reporte de clasificaci√≥n y la matriz de confusi√≥n, podemos observar:</p>
            
            <ul>
                <li><strong>Exactitud (Accuracy):</strong> 79%, lo que indica que el modelo clasifica correctamente aproximadamente 4 de cada 5 pasajeros.</li>
                <li><strong>Clase No Sobrevive (False):</strong> Alta sensibilidad (94%) pero precisi√≥n m√°s baja (77%), lo que significa que el modelo es muy bueno identificando a los que no sobrevivieron, pero tiende a clasificar incorrectamente algunos sobrevivientes como no sobrevivientes.</li>
                <li><strong>Clase Sobrevive (True):</strong> Alta precisi√≥n (86%) pero sensibilidad m√°s baja (55%), lo que indica que cuando el modelo predice que alguien sobrevive, generalmente acierta, pero tiende a perder casi la mitad de los casos de sobrevivientes reales.</li>
            </ul>
            
            <p>Este patr√≥n de rendimiento es interesante: el modelo es conservador al predecir supervivencia, lo que resulta en menos falsos positivos pero m√°s falsos negativos.</p>

        </div>

        <div class="content-section">
            <h3 id="etapa-12-interpretacion">Etapa 12: Interpretaci√≥n del Modelo</h3>
            
            <p>Una ventaja clave de los √°rboles de decisi√≥n es su interpretabilidad. Vamos a examinar qu√© caracter√≠sticas son m√°s importantes para nuestro modelo:</p>
            
<pre><code class="language-python"># Extraer y visualizar la importancia de las caracter√≠sticas
dfFeatures = pd.DataFrame({'Features': decision_tree_model['preprocessor'].get_feature_names_out(),
                          'Importances': decision_tree_model['model'].feature_importances_})

# Ordenar por importancia
dfFeatures = dfFeatures.sort_values(by='Importances', ascending=False)

# Mostrar las caracter√≠sticas m√°s importantes
print("Caracter√≠sticas m√°s importantes:")
print(dfFeatures)</code></pre>
            
<pre class="output"><code class="language-shell">Caracter√≠sticas m√°s importantes:
                        Features  Importances
5          categoricas__sex_male     0.454508
1                numericas__fare     0.187830
0                 numericas__age     0.135766
2               numericas__sibsp     0.063084
3               numericas__parch     0.060102
9  categoricas ordinales__pclass     0.057476
6        categoricas__embarked_C     0.026747
8        categoricas__embarked_S     0.011760
7        categoricas__embarked_Q     0.002729
4        categoricas__sex_female     0.000000</code></pre>
            
            <p>Visualizamos estas importancias para facilitar su interpretaci√≥n:</p>
            
<pre><code class="language-python"># Visualizar importancia de caracter√≠sticas
plt.figure(figsize=(10, 6))
sns.barplot(x='Importances', y='Features', data=dfFeatures)
plt.title('Importancia de Caracter√≠sticas - Decision Tree')
plt.tight_layout()
plt.show()</code></pre>
            
            <div class="img-container">
                <img src="https://github.com/AntonioEscamilla/images-in-readMe/raw/master/Curso%20Python/titanic_8.png" alt="Importancia de caracter√≠sticas en Decision Tree" width="70%">
                <div class="caption">Importancia relativa de las caracter√≠sticas en el modelo Decision Tree</div>
            </div>
            
            <h3>An√°lisis de la Importancia de Caracter√≠sticas</h3>
            
            <p>El an√°lisis de importancia revela insights valiosos sobre los factores que determinaron la supervivencia en el desastre del Titanic:</p>
            
            <ul>
                <li><strong>Sexo (sex_male):</strong> Con una importancia del 45.5%, esta es por mucho la caracter√≠stica m√°s influyente. Confirma hist√≥ricamente la pol√≠tica de "mujeres y ni√±os primero" implementada durante la evacuaci√≥n.</li>
                <li><strong>Tarifa pagada (fare):</strong> Segunda caracter√≠stica m√°s importante (18.8%), sugiriendo que el costo del boleto, que refleja la clase socioecon√≥mica, tuvo un impacto significativo en las probabilidades de supervivencia.</li>
                <li><strong>Edad (age):</strong> Con una importancia del 13.6%, confirma que la edad fue un factor relevante en la supervivencia, probablemente favoreciendo a los ni√±os durante la evacuaci√≥n.</li>
                <li><strong>Relaciones familiares (sibsp y parch):</strong> Con una importancia combinada del 12.3%, indica que viajar con familia (hermanos/c√≥nyuges o padres/hijos) afect√≥ las probabilidades de supervivencia.</li>
                <li><strong>Clase del pasajero (pclass):</strong> Con un 5.7% de importancia, tuvo un impacto moderado. Es interesante notar que parte de su efecto ya est√° capturado en la variable 'fare'.</li>
                <li><strong>Puerto de embarque (embarked):</strong> Los puertos de embarque tienen una influencia menor (4.1% combinado).</li>
            </ul>
            
            <p>En base a estos resultados, podr√≠amos probar un modelo simplificado que utilice solo las caracter√≠sticas m√°s importantes. Esto podr√≠a mejorar la generalizaci√≥n del modelo y hacerlo m√°s eficiente.</p>
            
            <div class="note">
                En los √°rboles de decisi√≥n, la importancia de las caracter√≠sticas se calcula seg√∫n cu√°nto mejora cada caracter√≠stica la pureza de los nodos (medida por el √≠ndice Gini o la entrop√≠a). Caracter√≠sticas con mayor importancia contribuyen m√°s a las decisiones cr√≠ticas del √°rbol.
            </div>
            
            <h3>Modelo Simplificado con Caracter√≠sticas Importantes</h3>
            
            <p>Vamos a probar si podemos obtener un rendimiento similar utilizando solo las caracter√≠sticas m√°s importantes:</p>
            
<pre><code class="language-python"># Utilizar solo las caracter√≠sticas principales: sexo, tarifa, edad y clase
X_train_reduced = X_train[['sex', 'fare', 'age', 'pclass']]
X_test_reduced = X_test[['sex', 'fare', 'age', 'pclass']]

# Actualizar las definiciones de columnas
cols_numericas = ["age", "fare"]
cols_categoricas = ["sex"]
cols_categoricas_ord = ["pclass"]

# Recrear el preprocesador
preprocessor_reduced = ColumnTransformer(
    transformers=[
        ('numericas', numeric_pipe, cols_numericas),
        ('categoricas', categorical_pipe, cols_categoricas),
        ('categoricas ordinales', categorical_ord_pipe, cols_categoricas_ord)
    ])

# Pipeline con modelo simplificado
simplified_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor_reduced),
    ("model", DecisionTreeClassifier(criterion='gini', max_depth=9, max_features=3, random_state=42))
])

# Entrenar y evaluar
simplified_pipe.fit(X_train_reduced, y_train)
y_pred_simplified = simplified_pipe.predict(X_test_reduced)
print("Reporte del modelo simplificado:")
print(classification_report(y_test, y_pred_simplified))</code></pre>
            
<pre class="output"><code class="language-shell">Reporte del modelo simplificado:
              precision    recall  f1-score   support

       False       0.82      0.90      0.86       162
        True       0.81      0.67      0.73       100

    accuracy                           0.81       262
   macro avg       0.81      0.79      0.79       262
weighted avg       0.81      0.81      0.81       262</code></pre>
            
            <p>¬°Notable! El modelo simplificado no solo mantiene el rendimiento, sino que incluso lo mejora ligeramente. Esto demuestra un principio importante en machine learning: a veces menos es m√°s. Un modelo m√°s simple puede generalizar mejor, evitar el sobreajuste y ser m√°s f√°cil de interpretar y mantener.</p>
        </div>

        <div class="content-section">
            <h3 id="etapa-13-preparacion">Etapa 13: Preparaci√≥n para Despliegue</h3>
            
            <p>Una vez finalizado el desarrollo y la evaluaci√≥n del modelo, el √∫ltimo paso es prepararlo para su despliegue en producci√≥n. Esto implica serializar (guardar) el modelo para poder utilizarlo posteriormente sin necesidad de reentrenarlo:</p>
            
<pre><code class="language-python">from joblib import dump, load

# Entrenar el modelo final con todos los datos disponibles
final_pipe.fit(X_features, y_target)

# Guardar el modelo entrenado
dump(final_pipe, 'titanic_survival_model.joblib')
print("Modelo guardado correctamente.")

# Ejemplo de c√≥mo cargar y utilizar el modelo guardado
loaded_model = load('titanic_survival_model.joblib')

# Crear un ejemplo para predecir
example = pd.DataFrame({
    'pclass': [1],
    'sex': ['female'],
    'age': [29.0],
    'sibsp': [0],
    'parch': [0],
    'fare': [211.34],
    'embarked': ['S']
})

# Realizar la predicci√≥n
prediction = loaded_model.predict(example)
proba = loaded_model.predict_proba(example)

print(f"Predicci√≥n: {'Sobrevive' if prediction[0] else 'No sobrevive'}")
print(f"Probabilidad de supervivencia: {proba[0][1]:.2f}")</code></pre>
            
<pre class="output"><code class="language-shell">Modelo guardado correctamente.
Predicci√≥n: Sobrevive
Probabilidad de supervivencia: 0.93</code></pre>
            
            <h3>Conclusiones del Proyecto</h3>
            
            <p>A lo largo de este workflow hemos seguido todos los pasos esenciales de un proyecto de clasificaci√≥n:</p>
            
            <ol>
                <li><strong>Preparaci√≥n y exploraci√≥n de datos</strong>: Hemos comprendido las caracter√≠sticas del conjunto de datos, tratado los valores faltantes y convertido las variables a los tipos adecuados.</li>
                <li><strong>An√°lisis exploratorio</strong>: Visualizamos las relaciones entre nuestras variables para entender mejor el problema.</li>
                <li><strong>Preprocesamiento</strong>: Aplicamos t√©cnicas de imputaci√≥n y codificaci√≥n para preparar nuestros datos para los algoritmos de machine learning.</li>
                <li><strong>Selecci√≥n de modelos</strong>: Probamos diferentes algoritmos y utilizamos validaci√≥n cruzada para comparar su rendimiento.</li>
                <li><strong>Optimizaci√≥n de hiperpar√°metros</strong>: Mejoramos el rendimiento de nuestros modelos mediante la b√∫squeda de los mejores hiperpar√°metros.</li>
                <li><strong>Evaluaci√≥n final</strong>: Utilizamos el conjunto de prueba para obtener una estimaci√≥n insesgada del rendimiento de nuestro modelo.</li>
                <li><strong>Interpretaci√≥n</strong>: Analizamos la importancia de las caracter√≠sticas para entender mejor el problema.</li>
                <li><strong>Despliegue</strong>: Preparamos el modelo para su uso en producci√≥n.</li>
            </ol>
            
            <p>El modelo final basado en regresi√≥n log√≠stica alcanz√≥ una precisi√≥n del 77% en el conjunto de prueba, con un puntaje F1 de 0.77. Las caracter√≠sticas m√°s importantes para predecir la supervivencia fueron el sexo, la edad, la tarifa y la clase del pasajero.</p>
            
            <p>Este enfoque estructurado puede aplicarse a cualquier problema de clasificaci√≥n, adaptando las t√©cnicas espec√≠ficas seg√∫n la naturaleza de los datos y los requisitos del problema.</p>
        </div>

        <div class="content-section">
            <h2 id="material-practica">Material de Pr√°ctica</h2>

            <p>Para consolidar los conceptos aprendidos en este cap√≠tulo, te invitamos a realizar los siguientes ejercicios pr√°cticos:</p>

            <ul class="resources-list">
                <li><strong><a href="https://colab.research.google.com/drive/1mPWz5i9LYEcPlhpx7r9dfGbGGMvo2RSu" target="_blank">Construcci√≥n de un proyecto de clasificaci√≥n completo</a></strong> - Notebook con el flujo de trabajo completo presentado en este cap√≠tulo.</li>
            </ul>
            
            <div class="tip">
                <strong>Ejercicio pr√°ctico:</strong> Intenta replicar el flujo de trabajo presentado en este cap√≠tulo con un dataset diferente, como el dataset de Digits o Iris disponibles en scikit-learn. Compara el rendimiento de diferentes modelos y analiza qu√© caracter√≠sticas son m√°s importantes en cada caso.
            </div>

            <h2 id="referencias">Referencias</h2>

            <p>Para profundizar en los conceptos de clasificaci√≥n y t√©cnicas relacionadas, recomendamos consultar estos recursos:</p>

            <ul class="resources-list">
                <li><strong><a href="https://scikit-learn.org/stable/supervised_learning.html" target="_blank">Documentaci√≥n oficial de modelos de aprendizaje supervisado en scikit-learn</a></strong> - Referencia completa sobre implementaci√≥n de modelos supervisados en Python.</li>
                <li><strong><a href="https://www.kaggle.com/datasets/marshalpatel3558/diabetes-prediction-dataset-legit-dataset" target="_blank">Competencia de Kaggle: Diabetes Prediction</a></strong> - Competencia popular para practicar t√©cnicas avanzadas de clasificaci√≥n.</li>
                <li><strong><a href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py" target="_blank">Comparaci√≥n de clasificadores en scikit-learn</a></strong> - Ejemplo de comparaci√≥n de diferentes modelos de clasificaci√≥n.</li>
            </ul>
        </div>
        
        <div class="content-section">
            <div class="feedback-section">
                <h3>¬øTe ha resultado √∫til esta p√°gina?</h3>
                <div class="feedback-buttons">
                    <button class="btn feedback-btn feedback-yes">
                        <span class="feedback-icon">üòä</span> S√≠
                    </button>
                    <button class="btn feedback-btn feedback-no">
                        <span class="feedback-icon">ü§î</span> No
                    </button>
                </div>
            </div>
     
            <!-- Nueva secci√≥n de navegaci√≥n entre cap√≠tulos -->
            <div class="chapter-navigation">
                <a href="regression.html" class="nav-btn nav-prev">
                    <span class="nav-icon">‚óÄ</span> Cap√≠tulo Anterior
                </a>
                <a href="clustering.html" class="nav-btn nav-next">
                    Cap√≠tulo Siguiente <span class="nav-icon">‚ñ∂</span>
                </a>
            </div>
        </div>
     
        <div class="footer">
            <p>¬© 2025 Python para Ciencia de Datos - Ph.D. Antonio Escamilla P.</p>
     
            <div class="license-icons">
                <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" target="_blank" class="license-link" title="Creative Commons BY-NC-ND 4.0">
                    <!-- √çcono CC -->
                    <span class="icon-tooltip" data-tooltip="Creative Commons">
                        <svg class="license-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="11" fill="var(--primary-medium)"/>
                            <circle cx="12" cy="12" r="10" fill="none" stroke="var(--accent-green)" stroke-width="1"/>
                            <text x="12" y="14" font-family="Arial, sans-serif" font-size="8" font-weight="bold" text-anchor="middle" fill="var(--accent-green)">CC</text>
                        </svg>
                    </span>
                    
                    <!-- √çcono BY (Atribuci√≥n) -->
                    <span class="icon-tooltip" data-tooltip="Atribuci√≥n">
                        <svg class="license-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="11" fill="var(--primary-medium)"/>
                            <circle cx="12" cy="12" r="10" fill="none" stroke="var(--accent-green)" stroke-width="1"/>
                            <circle cx="12" cy="8" r="2.5" fill="var(--accent-green)"/>
                            <path d="M8,16 L16,16 L16,13 C16,11.5 14,11 12,11 C10,11 8,11.5 8,13 L8,16 Z" fill="var(--accent-green)"/>
                        </svg>
                    </span>
                    
                    <!-- √çcono NC (No Comercial) -->
                    <span class="icon-tooltip" data-tooltip="No Comercial">
                        <svg class="license-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="11" fill="var(--primary-medium)"/>
                            <circle cx="12" cy="12" r="10" fill="none" stroke="var(--accent-green)" stroke-width="1"/>
                            <text x="12" y="15" font-family="Arial, sans-serif" font-size="10" font-weight="bold" text-anchor="middle" fill="var(--accent-green)">$</text>
                            <line x1="6" y1="6" x2="18" y2="18" stroke="var(--accent-green)" stroke-width="2"/>
                        </svg>
                    </span>
                    
                    <!-- √çcono ND (Sin Derivadas) -->
                    <span class="icon-tooltip" data-tooltip="Sin Derivadas">
                        <svg class="license-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="11" fill="var(--primary-medium)"/>
                            <circle cx="12" cy="12" r="10" fill="none" stroke="var(--accent-green)" stroke-width="1"/>
                            <rect x="7" y="10" width="10" height="1.5" fill="var(--accent-green)"/>
                            <rect x="7" y="13" width="10" height="1.5" fill="var(--accent-green)"/>
                        </svg>
                    </span>
                </a>
            </div>
        </div>
    </div>
     
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Aplicar resaltado a todos los bloques de c√≥digo
            document.querySelectorAll('pre code').forEach(function(block) {
                hljs.highlightElement(block);
            });
            
            const hamburgerMenu = document.querySelector('.hamburger-menu');
            const hamburgerButton = document.querySelector('.hamburger-button');
            
            // Alternar men√∫ al hacer clic en el bot√≥n
            hamburgerButton.addEventListener('click', function() {
                hamburgerMenu.classList.toggle('active');
            });
     
            // Cerrar men√∫ al hacer clic en un enlace
            const menuLinks = document.querySelectorAll('.menu-content a');
            menuLinks.forEach(link => {
                link.addEventListener('click', function() {
                    hamburgerMenu.classList.remove('active');
                });
            });
     
            // Cerrar men√∫ al hacer clic fuera de √©l
            document.addEventListener('click', function(event) {
                if (!hamburgerMenu.contains(event.target)) {
                    hamburgerMenu.classList.remove('active');
                }
            });
        });
    </script>
</body>
